{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ea1c1c",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fd3c50",
   "metadata": {},
   "source": [
    "Consider the following document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b3ca24ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer.vocabulary_: {'the': 85, 'rank': 70, 'of': 58, 'matrix': 49, 'is': 41, 'maximum': 50, 'number': 57, 'linearly': 46, 'independent': 38, 'columns': 11, 'graph': 30, 'function': 26, 'in': 37, 'two': 90, 'variable': 92, 'surface': 83, '3d': 0, 'space': 80, 'partial': 64, 'derivative': 18, 'ordinary': 62, 'eigenvalues': 25, 'are': 6, 'roots': 71, 'characteristic': 9, 'polynomial': 66, 'gradient': 29, 'variables': 93, 'contains': 13, 'its': 43, 'derivatives': 19, 'slope': 79, 'tangent': 84, 'plane': 65, 'if': 36, 'convex': 15, 'then': 86, 'any': 4, 'local': 47, 'minimizer': 52, 'global': 27, 'steepest': 82, 'descent': 20, 'method': 51, 'uses': 91, 'opposite': 60, 'direction': 22, 'globally': 28, 'convergent': 14, 'wolfe': 97, 'condition': 12, 'holds': 34, 'hessian': 33, 'second': 74, 'order': 61, 'inverse': 40, 'an': 2, 'orthogonal': 63, 'transpose': 88, 'singular': 78, 'it': 42, 'has': 31, 'at': 7, 'least': 44, 'one': 59, 'null': 56, 'eigenvalue': 24, 'product': 68, 'and': 3, 'traspose': 89, 'identity': 35, 'linear': 45, 'approximation': 5, 'column': 10, 'called': 8, 'range': 69, 'rows': 72, 'set': 76, 'vectors': 95, 'spectrum': 81, 'all': 1, 'distinct': 23, 'normed': 54, 'vectorial': 94, 'with': 96, 'inner': 39, 'norm': 53, 'similar': 77, 'matrices': 48, 'have': 32, 'same': 73, 'positive': 67, 'semi': 75, 'definite': 17, 'not': 55, 'to': 87, 'diagonal': 21, 'defective': 16}\n",
      "(98, 23)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import sklearn.feature_extraction\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(min_df=1)\n",
    "\n",
    "documents = [\n",
    "    'The rank of a matrix is the maximum number of linearly independent columns.',\n",
    "    'The graph of a function in two variable is a surface in the 3D space',\n",
    "    'The partial derivative is the ordinary derivative of a partial function',\n",
    "    'The eigenvalues of a matrix are the roots of the characteristic polynomial.',       \n",
    "    'The gradient of a two-variables function contains its partial derivatives',\n",
    "    'The partial derivatives are the slope of the tangent plane',\n",
    "    'If a function is convex then any local minimizer is a global minimizer',\n",
    "    'A steepest descent method uses the opposite of the gradient direction',\n",
    "    'The steepest descent method is globally convergent if the Wolfe condition holds',\n",
    "    'The Hessian matrix is the matrix of second-order partial derivatives',\n",
    "    'The inverse of an orthogonal matrix is its transpose.',\n",
    "    'If A is a singular matrix then it has at least one null eigenvalue.',\n",
    "    'The product of an orthogonal matrix and its traspose is the identity matrix.',\n",
    "    'The tangent plane is the linear approximation of a surface',\n",
    "    'The column space of a matrix A is called range of A.',\n",
    "    'The rank of a matrix is the maximum number of  linearly independent rows.',\n",
    "    'A set of orthogonal vectors is a linearly independent set.',\n",
    "    'The spectrum of a matrix is the set of all its distinct eigenvalues.',\n",
    "    'The columns of an orthogonal matrix are a set of orthogonal vectors.',\n",
    "    'A normed vectorial space is a space with an inner product norm.',\n",
    "    'Similar matrices have the same spectrum.',\n",
    "    'The Hessian matrix of a convex function is positive semi-definite',\n",
    "    'Matrices not similar to a diagonal matrix are called defective.'\n",
    "]\n",
    "\n",
    "X = vectorizer.fit_transform(documents).toarray()\n",
    "print('vectorizer.vocabulary_: {0}'.format(vectorizer.vocabulary_))\n",
    "\n",
    "A = X.T\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8942addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64cb8d0",
   "metadata": {},
   "source": [
    "To remove the stop words and perform stemming you can download the package \"gensim\" by uncommenting the following instruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8462f0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc92e167",
   "metadata": {},
   "source": [
    "Then, after you installed \"gensim\" you can proceed with the following instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9cecf944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords, stem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fddbd579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " List of documents without stop words\n",
      "  ['The rank matrix maximum number linearly independent columns.', 'The graph function variable surface 3D space', 'The partial derivative ordinary derivative partial function', 'The eigenvalues matrix roots characteristic polynomial.', 'The gradient two-variables function contains partial derivatives', 'The partial derivatives slope tangent plane', 'If function convex local minimizer global minimizer', 'A steepest descent method uses opposite gradient direction', 'The steepest descent method globally convergent Wolfe condition holds', 'The Hessian matrix matrix second-order partial derivatives', 'The inverse orthogonal matrix transpose.', 'If A singular matrix null eigenvalue.', 'The product orthogonal matrix traspose identity matrix.', 'The tangent plane linear approximation surface', 'The column space matrix A called range A.', 'The rank matrix maximum number linearly independent rows.', 'A set orthogonal vectors linearly independent set.', 'The spectrum matrix set distinct eigenvalues.', 'The columns orthogonal matrix set orthogonal vectors.', 'A normed vectorial space space inner product norm.', 'Similar matrices spectrum.', 'The Hessian matrix convex function positive semi-definite', 'Matrices similar diagonal matrix called defective.']\n",
      "\n",
      " List of documents after stemming \n",
      "  ['the rank matrix maximum number linearli independ columns.', 'the graph function variabl surfac 3d space', 'the partial deriv ordinari deriv partial function', 'the eigenvalu matrix root characterist polynomial.', 'the gradient two-vari function contain partial deriv', 'the partial deriv slope tangent plane', 'if function convex local minim global minim', 'a steepest descent method us opposit gradient direct', 'the steepest descent method global converg wolf condit hold', 'the hessian matrix matrix second-ord partial deriv', 'the invers orthogon matrix transpose.', 'if a singular matrix null eigenvalue.', 'the product orthogon matrix traspos ident matrix.', 'the tangent plane linear approxim surfac', 'the column space matrix a call rang a.', 'the rank matrix maximum number linearli independ rows.', 'a set orthogon vector linearli independ set.', 'the spectrum matrix set distinct eigenvalues.', 'the column orthogon matrix set orthogon vectors.', 'a norm vectori space space inner product norm.', 'similar matric spectrum.', 'the hessian matrix convex function posit semi-definit', 'matric similar diagon matrix call defective.']\n"
     ]
    }
   ],
   "source": [
    "L = []\n",
    "for i in range(len(documents)):\n",
    "    L.append(remove_stopwords(documents[i]))\n",
    "print('\\n List of documents without stop words\\n ', L)\n",
    "\n",
    "L_final = []\n",
    "for i in range(len(documents)):\n",
    "    L_final.append(stem_text(L[i]))\n",
    "print('\\n List of documents after stemming \\n ', L_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b51b2f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer.vocabulary_: {'the': 66, 'rank': 52, 'matrix': 35, 'maximum': 36, 'number': 41, 'linearli': 32, 'independ': 28, 'columns': 5, 'graph': 23, 'function': 20, 'variabl': 72, 'surfac': 64, '3d': 0, 'space': 61, 'partial': 46, 'deriv': 12, 'ordinari': 44, 'eigenvalu': 17, 'root': 53, 'characterist': 3, 'polynomial': 48, 'gradient': 22, 'two': 69, 'vari': 71, 'contain': 7, 'slope': 60, 'tangent': 65, 'plane': 47, 'if': 27, 'convex': 9, 'local': 33, 'minim': 38, 'global': 21, 'steepest': 63, 'descent': 13, 'method': 37, 'us': 70, 'opposit': 42, 'direct': 15, 'converg': 8, 'wolf': 76, 'condit': 6, 'hold': 25, 'hessian': 24, 'second': 55, 'ord': 43, 'invers': 30, 'orthogon': 45, 'transpose': 67, 'singular': 59, 'null': 40, 'eigenvalue': 18, 'product': 50, 'traspos': 68, 'ident': 26, 'linear': 31, 'approxim': 1, 'column': 4, 'call': 2, 'rang': 51, 'rows': 54, 'set': 57, 'vector': 73, 'spectrum': 62, 'distinct': 16, 'eigenvalues': 19, 'vectors': 75, 'norm': 39, 'vectori': 74, 'inner': 29, 'similar': 58, 'matric': 34, 'posit': 49, 'semi': 56, 'definit': 11, 'diagon': 14, 'defective': 10}\n",
      "(77, 23)\n"
     ]
    }
   ],
   "source": [
    "Y = vectorizer.fit_transform(L_final).toarray()\n",
    "print('vectorizer.vocabulary_: {0}'.format(vectorizer.vocabulary_))\n",
    "\n",
    "A = Y.T\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ff9e4",
   "metadata": {},
   "source": [
    "Consider the query vector ``gradient''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "403da06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query vector\n",
    "query1text = ['gradient']\n",
    "query_stem = query1text[0]#stem_text(query1text[0])\n",
    "query1 = []\n",
    "query1.append(query_stem)\n",
    "query1 = vectorizer.transform(query1).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d92407",
   "metadata": {},
   "source": [
    "As you can see the vector ``query1`` is a row vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9d758332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 77)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539339c0",
   "metadata": {},
   "source": [
    "The search for relevant documents is carried out by computing the cosines of the angles  between the query\n",
    "vector and the document vectors. A document is returned as relevant only if the cosine of such an angle is greater than some threshold or cutoff value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aef53ce",
   "metadata": {},
   "source": [
    "Compute:\n",
    "\n",
    "- The \"cosine similarity\" between the query vector ``query1`` and the columns of $A$\n",
    "- The \"cosine similarity\" between the query vector ``query1`` and an orthogonal basis of the column space of $A$ (use the QR factorization with pivot: ``[Q,R,P]=scipy.linalg.qr(A,mode='economic',pivoting=True)`` ). In particular derermine a suitable subspace spanned by $k$ elements rather than the full set of $rank(A)$ elements by setting the tolerance to ``0.92`` and choosing $k$ following these instructions:\n",
    " \n",
    "  1) extract the diagonal elements of ``R`` and copy them in an auxiliary vector ``Rdiag``;\n",
    "    \n",
    "  2) scale the absolute values of ``Rdiag`` with respect to its absolute maximum\n",
    "    \n",
    "  3) compute $k$ as  the number of elements of ``Rdiag`` that are greather then the chosen tolerance\n",
    "  \n",
    "- Then perform the Latent Semantic Index to compute the cosine of the angles. Choose one of the showed techniques for computing  a suitable number $k$ of components. \n",
    "Show the error with respect to the cosine similarity computed using the full matriz $A$ and discuss the obtained results.\n",
    "\n",
    "- Using the singular value decomposition, compute the closest point  to the query in the range of $A$ and in the range of $A_k$. \n",
    "\n",
    "- Choose another query vector ``query2`` at your discretion and repeat the previous steps. \n",
    "\n",
    "- Construct the column-covariance matrix, numerically determine if it is positive definite. Do the eigenvalues of such a matrix satisfy the theoretical relation with the singular value of the matrix $A$? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28333286",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "> Compute the \"cosine similarity\" between the query vector `query1` and the columns of $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad22339",
   "metadata": {},
   "source": [
    "Before doing any computation, we compute the 2-norm of every column of the matrix $A$ and the query to normalize all the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "20f27505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-norm of matrix N columns =\n",
      " [2.82842712 2.64575131 3.31662479 2.44948974 2.82842712 2.44948974\n",
      " 3.         2.64575131 3.         3.16227766 2.23606798 2.23606798\n",
      " 3.         2.44948974 2.44948974 2.82842712 2.82842712 2.44948974\n",
      " 3.         3.31662479 1.73205081 2.82842712 2.44948974]\n"
     ]
    }
   ],
   "source": [
    "# normalize matrix A with the norm of each column\n",
    "(m, n) = A.shape\n",
    "norm = np.array(np.zeros(n))\n",
    "\n",
    "for i in range(n):\n",
    "    norm[i] = np.linalg.norm(A[:, i], 2)\n",
    "\n",
    "# normalize matrix A\n",
    "N = np.dot(A, np.diag(1/norm))\n",
    "\n",
    "print('1-norm of matrix N columns =\\n', norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c2a3f6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-norm of query1 = 1.0\n"
     ]
    }
   ],
   "source": [
    "# normalize query vector\n",
    "q = np.linalg.norm((query1), 2)\n",
    "print('2-norm of query1 =', q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0673d1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix N (rows, cols) = (77, 23)\n",
      "Shape of vector query1 (rows, cols) = (1, 77)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of matrix N (rows, cols) =', N.shape)\n",
    "print('Shape of vector query1 (rows, cols) =', query1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c0ad3",
   "metadata": {},
   "source": [
    "In Text Mining tasks, the cosine similarity is a measure of similarity between non-zero vectors given by the cosine of the angle between them.\n",
    "\n",
    "In Linear Algebra, given an inner-product space $V$, the radian measure of the angle between non-zero vectors $x, y \\in V$ is defined to be the number $\\theta \\in [0, \\pi]$ such that\n",
    "\n",
    "$$cos(\\theta) = \\frac{x^T y}{\\|x\\| \\|y\\|}$$\n",
    "\n",
    "where $x^T y$ is the inner-product between $x$ and $y$.\n",
    "\n",
    "In our case, our vector `query1` and the columns of our term-document matrix `N` belong to the same inner-product space, the vocabulary that we have built during the preprocessing operations. We can compute, for each column of `N`, the value of $cos(\\theta)$ between `query1` and every document. By substituting the terms in the previous formula, we get\n",
    "\n",
    "$$cos(\\theta) = \\frac{n^T q}{\\|n\\| \\|q\\|}$$\n",
    "\n",
    "where $n^T$ is the traspose of a column of `N` and $q$ is our query `query1`.\n",
    "\n",
    "As we have already normalized the columns of the matrix `N` and our query `query1`, we need to compute\n",
    "\n",
    "$$cos(\\theta) = n^T q$$\n",
    "\n",
    "for every column $n$ in `N`.\n",
    "\n",
    "To do the inner-product between a matrix and a vector, they should be conformable in their shapes. We know that the transpose of our term-document matrix is a $23 \\times 77$ matrix and, as we have built our query vector, we need to transpose it to get a $77 \\times 1$ vector. After the computation of $cos(\\theta)$, we will get a $23 \\times 1$ vector with the value of the cosine of the angle between every document and the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a7185470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of cos_theta (rows, cols) = (23, 1)\n",
      "\n",
      "cos(theta) between document and query\n",
      " [[0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.35355339]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.37796447]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# we do the transpose of the matrix and the vector so they are conformable for matrix-vector product\n",
    "cos_theta = np.dot(N.T, query1.T)\n",
    "\n",
    "print('Shape of cos_theta (rows, cols) =', cos_theta.shape)\n",
    "print('\\ncos(theta) between document and query\\n', cos_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc2c8c1",
   "metadata": {},
   "source": [
    "Selecting the **maximum absolute value of the cosine** of the angle between the documents and the query is the \"answer\" to our \"question\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "019359dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: gradient\n",
      "\n",
      "Best answer\n",
      " A steepest descent method uses the opposite of the gradient direction\n",
      "\n",
      "Index of the document: 7\n"
     ]
    }
   ],
   "source": [
    "doc_idx = np.argmax(np.abs(cos_theta))\n",
    "\n",
    "print('Question:', query1text[0])\n",
    "print('\\nBest answer\\n', documents[doc_idx])\n",
    "print('\\nIndex of the document:', doc_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3889cb5b",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "> The \"cosine similarity\" between the query vector ``query1`` and an orthogonal basis of the column space of $A$ (use the QR factorization with pivot: ``[Q,R,P]=scipy.linalg.qr(A,mode='economic',pivoting=True)`` ). In particular derermine a suitable subspace spanned by $k$ elements rather than the full set of $rank(A)$ elements by setting the tolerance to ``0.92`` and choosing $k$ following these instructions:\n",
    "1. extract the diagonal elements of ``R`` and copy them in an auxiliary vector ``Rdiag``\n",
    "2. scale the absolute values of ``Rdiag`` with respect to its absolute maximum\n",
    "3. compute $k$ as  the number of elements of ``Rdiag`` that are greather then the chosen tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ea13ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60db5c63",
   "metadata": {},
   "source": [
    "As we have performed the normalization of the columns in the [previous task](#task-1), we copy our results inside a new matrix `A` to use them in every next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0a86314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.copy(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bef13c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Q (rows, cols) = (77, 23)\n",
      "Size of R (rows, cols) = (23, 23)\n",
      "Size of P (rows, cols) = (23,)\n"
     ]
    }
   ],
   "source": [
    "Q, R, P = sp.linalg.qr(A, mode='economic', pivoting='True')\n",
    "print('Size of Q (rows, cols) =', Q.shape)\n",
    "print('Size of R (rows, cols) =', R.shape)\n",
    "print('Size of P (rows, cols) =', P.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe5fe4",
   "metadata": {},
   "source": [
    "Every matrix $A^{m \\times n}$ with linearly independent columns can be uniquely factored as\n",
    "\n",
    "$$A = QR$$\n",
    "\n",
    "in which the columns of $Q^{m \\times n}$ are an orthonormal basis for $R(A)$ and $R^{n \\times n}$ is an upper-triangular matrix with positive diagonal entries.\n",
    "\n",
    "In our case\n",
    "\n",
    "$$A^{77 \\times 23} P^{23 \\times 1} = Q^{23 \\times 23} R^{23 \\times 23}$$\n",
    "\n",
    "because we have performed the QR routine from `scipy` using `mode='economic'` that returns $Q^{m \\times k}$ and $R^{k \\times n}$ with $k = min(m, n)$ that are the number of rows and columns of $A$, respectively. It also have performed a pivoting by using `pivoting=True`, choosing $P$ such that the diagonal elements of R are ordered, non-increasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "db1bc528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagonal elements of R\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      " -0.99239533 -0.98362664 -0.96267198 -0.9098435  -0.89573912 -0.89421085\n",
      "  0.87664797  0.86030769 -0.83640726 -0.80785277 -0.76780099  0.73695401\n",
      "  0.70834723  0.64449965  0.63875837  0.6204548   0.47789138]\n"
     ]
    }
   ],
   "source": [
    "# 1. Extract the diagonal elements of R and copy them in Rdiag\n",
    "Rdiag = np.diag((R))\n",
    "print('Diagonal elements of R\\n', Rdiag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a4858ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagonal elements of R, scaled\n",
      " [1.         1.         1.         1.         1.         1.\n",
      " 0.99239533 0.98362664 0.96267198 0.9098435  0.89573912 0.89421085\n",
      " 0.87664797 0.86030769 0.83640726 0.80785277 0.76780099 0.73695401\n",
      " 0.70834723 0.64449965 0.63875837 0.6204548  0.47789138]\n"
     ]
    }
   ],
   "source": [
    "# 2. Scale the absolute values of Rdiag with respect to its absolute maximum\n",
    "abs_max = np.max(np.abs(Rdiag))\n",
    "Rdiag_scaled = np.abs(Rdiag)/abs_max\n",
    "\n",
    "print('Diagonal elements of R, scaled\\n', Rdiag_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "53218c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank(A) = 23\n",
      "k = 9\n"
     ]
    }
   ],
   "source": [
    "# 3. Compute k as the number of elements of Rdiag that are greather then the chosen tolerance\n",
    "tol = 0.92\n",
    "k = np.sum(Rdiag_scaled > tol)\n",
    "\n",
    "A_rank = np.linalg.matrix_rank(A)\n",
    "\n",
    "print('rank(A) =', A_rank)\n",
    "print('k =', k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade87da1",
   "metadata": {},
   "source": [
    "Noticing that $k < rank(A) = 23$, we select a smaller orthogonal basis for the column space $R(A)$ extracting $k$ columns from the matrix $Q$, obtained from the QR factorization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "885bb9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthogonal basis (rows, cols) = (77, 9)\n"
     ]
    }
   ],
   "source": [
    "# Find an orthogonal basis for the column space from the QR factorization\n",
    "orth_basis = Q[:, :k]\n",
    "\n",
    "print('Orthogonal basis (rows, cols) =', orth_basis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f74b5013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of cos_theta_qr (rows, cols) = (9, 1)\n",
      "\n",
      "Cosine of the angle between the query and the documents\n",
      " [[ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.37796447]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# we do the transpose of the matrix and the vector so they are conformable for matrix-vector product\n",
    "cos_theta_qr = np.dot(orth_basis.T, query1.T)\n",
    "\n",
    "print('Shape of cos_theta_qr (rows, cols) =', cos_theta_qr.shape)\n",
    "print('\\nCosine of the angle between the query and the documents\\n', cos_theta_qr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5830402a",
   "metadata": {},
   "source": [
    "By computing the cosine similarity between the columns of the orthogonal basis for the columns space of $A$ that we have selected with $k = 9$, we get a different result from the [previous task](#task-1), but always related to our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4e5be7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: gradient\n",
      "\n",
      "Best answer\n",
      " The gradient of a two-variables function contains its partial derivatives\n",
      "\n",
      "Index of the document: 4\n"
     ]
    }
   ],
   "source": [
    "doc_idx = np.argmax(np.abs(cos_theta_qr))\n",
    "\n",
    "print('Question:', query1text[0])\n",
    "print('\\nBest answer\\n', documents[doc_idx])\n",
    "print('\\nIndex of the document:', doc_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a4231f",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "> Then perform the Latent Semantic Index to compute the cosine of the angles. Choose one of the showed techniques for computing a suitable number $k$ of components. \n",
    "Show the error with respect to the cosine similarity computed using the full matrix $A$ and discuss the obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3af69e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.copy(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a95fd6d",
   "metadata": {},
   "source": [
    "The **Latent Semantic Index (LSI)**, also known as **Latent Semantic Analysis (LSA)** is an indexing and retrieval method that uses the **Singular Value Decomposition (SVD)** to identify patterns in the relationship between terms and concepts.\n",
    "\n",
    "For each $A \\in \\R^{m \\times n}$ of rank $r$, there are orthogonal matrices $U^{m \\times m}$ and $V^{n \\times n}$ and a diagonal matrix $D^{r \\times r} = diag(\\sigma_1, \\sigma_2, ..., \\sigma_r)$ such that\n",
    "\n",
    "$$A = U \\Sigma V^T = U \\begin{bmatrix} D & 0 \\\\ 0 & 0\\end{bmatrix}_{m \\times n} V^T$$\n",
    "\n",
    "with $\\sigma_1 \\ge \\sigma_2 \\ge ... \\ge \\sigma_r > 0$, that are called singular values of $A$.\n",
    "\n",
    "A **truncated SVD** can be useful to find out relevant information from \"noisy\" data. We refer to the truncated SVD as the following:\n",
    "\n",
    "$$A_k = \\sum_{i = 1}^k \\sigma_i u_i v_i^T$$\n",
    "\n",
    "Determining the best value of $k$ often requires empirical techniques, but looking for obvious gaps between large and small singular values is usually a good practice. After finding $k$ as significantly less than $r$, we can replace the SVD of $A$ by a low-rank truncation $A_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5be18e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of U (rows, cols) = (77, 23)\n",
      "Size of s (rows, cols) = (23,)\n",
      "Size of V (rows, cols) = (23, 23)\n"
     ]
    }
   ],
   "source": [
    "# Latent Semantic Index for cosine of the angles\n",
    "U, s, Vt = sp.linalg.svd(A, full_matrices=False)\n",
    "\n",
    "print('Size of U (rows, cols) =', U.shape)\n",
    "print('Size of s (rows, cols) =', s.shape)\n",
    "print('Size of V (rows, cols) =', Vt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b797db",
   "metadata": {},
   "source": [
    "To solve this task, we define here some useful functions and methods that will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0de36af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_rank_matrix(A, k):\n",
    "    \"\"\"\n",
    "    Compute a low-rank matrix of rank k using SVD on A\n",
    "    Input: matrix A, custom rank k\n",
    "    Output: low-rank matrix Ak with rank(A) = k\n",
    "    \"\"\"\n",
    "    U, s, Vt = sp.linalg.svd(A, full_matrices=False)\n",
    "    Ak = np.dot(U[:, 0:k]*s[0:k], Vt[0:k, :])\n",
    "\n",
    "    return Ak\n",
    "\n",
    "def check_relative_error(A, Ak, k):\n",
    "    \"\"\"\n",
    "    Compute the norm ||A-Ak||/||A|| and convert it in percentage\n",
    "    Input: matrix A, low-rank matrix Ak, rank k of Ak\n",
    "    Output: message\n",
    "    \"\"\"\n",
    "    rel_norm = np.linalg.norm(Ak - A) / np.linalg.norm(A)\n",
    "    print('Percentage relative error with k =', k, 'is', rel_norm*100, '%')\n",
    "\n",
    "def compute_cosine_similarity(A, query, query_text):\n",
    "    \"\"\"\n",
    "    Given a column-normalized matrix and a normalized vector compute cos(theta)\n",
    "    Input: column-normalized matrix A, normalized vector query, vector with the text of the query query_text\n",
    "    Output: cos_theta and a message\n",
    "    \"\"\"\n",
    "    cos_theta = np.dot(A.T, query.T)\n",
    "\n",
    "    doc_idx = np.argmax(np.abs(cos_theta))\n",
    "\n",
    "    print('Question:', query_text[0])\n",
    "    print('\\nBest answer\\n', documents[doc_idx])\n",
    "    print('\\nIndex of the document:', doc_idx)\n",
    "\n",
    "    return cos_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c275e9f0",
   "metadata": {},
   "source": [
    "To find $k$, we will compare the results of four different approaches:\n",
    "\n",
    "0. Full matrix\n",
    "1. Scree plot\n",
    "2. Kaiser rule\n",
    "3. Entropy based selection\n",
    "4. Cumulative percentage of total variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db98dba",
   "metadata": {},
   "source": [
    "### Full matrix\n",
    "\n",
    "This is the \"best\" case, as we are choosing $k = rank(A) = 23$. We expect a very low relative error and the \"best\" answer for our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8cbd341c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: gradient\n",
      "\n",
      "Best answer\n",
      " A steepest descent method uses the opposite of the gradient direction\n",
      "\n",
      "Index of the document: 7\n",
      "\n",
      "Percentage relative error with k = 23 is 5.918951029193003e-13 %\n"
     ]
    }
   ],
   "source": [
    "k = np.linalg.matrix_rank(A)\n",
    "\n",
    "A_full = low_rank_matrix(A, k)\n",
    "A_full_cos_theta = compute_cosine_similarity(A_full, query1, query1text)\n",
    "print()\n",
    "\n",
    "check_relative_error(A, A_full, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf0d560",
   "metadata": {},
   "source": [
    "### Scree plot\n",
    "\n",
    "By choosing the **Scree plot** approach, we have to plot $\\Sigma$ from the SVD to check the rate of decay of the singular values.\n",
    "\n",
    "Finding the \"elbow\" of the curve by eyes can be subjected to free interpretations. To find it, we can check the **rate of decay of the singular values** and select the one that shows that this rate is slowing down.\n",
    "\n",
    "A rapid drop in singular values followed by a gradual decrease often indicates that most of the important information in the data is **captured by the first few components**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3ed8daa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elbow point: k = 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# we compute the first derivative of the singular values\n",
    "derivative = np.diff(s)\n",
    "\n",
    "# and we find the index where the rate of change starts to decrease (the elbow point)\n",
    "elbow_index = np.argmax(derivative < 0) + 1\n",
    "\n",
    "print(f\"Elbow point: k = {elbow_index}\")\n",
    "\n",
    "plt.semilogy(s, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b16706",
   "metadata": {},
   "source": [
    "Using this approach, we should use $k=1$ and then check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "54a4aedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested k = 1\n",
      "\n",
      "Question: gradient\n",
      "\n",
      "Best answer\n",
      " The Hessian matrix is the matrix of second-order partial derivatives\n",
      "\n",
      "Index of the document: 9\n",
      "\n",
      "Percentage relative error with k = 1 is 88.11288732294778 %\n"
     ]
    }
   ],
   "source": [
    "print('Suggested k =', elbow_index)\n",
    "print()\n",
    "\n",
    "Ak_sp = low_rank_matrix(A, elbow_index)\n",
    "Ak_sp_cos_theta = compute_cosine_similarity(Ak_sp, query1, query1text)\n",
    "print()\n",
    "\n",
    "check_relative_error(A, Ak_sp, elbow_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e8963d",
   "metadata": {},
   "source": [
    "### Kaiser rule\n",
    "\n",
    "Applying the **Kaiser rule**, we retain all those singular values which exceed $1$. This approach often overestimates the number of singular values to keep and it also ignores the error due to sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "796b7333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested k = 9\n",
      "\n",
      "Question: gradient\n",
      "\n",
      "Best answer\n",
      " A steepest descent method uses the opposite of the gradient direction\n",
      "\n",
      "Index of the document: 7\n",
      "\n",
      "Percentage relative error with k = 9 is 53.61255813424427 %\n"
     ]
    }
   ],
   "source": [
    "# compute the mean variance\n",
    "mean_var = 1/A_rank * np.sum(s**2)\n",
    "k = np.sum(s**2 > mean_var)\n",
    "\n",
    "print('Suggested k =', k)\n",
    "print()\n",
    "\n",
    "Ak_kai = low_rank_matrix(A, k)\n",
    "Ak_kai_cos_theta = compute_cosine_similarity(Ak_kai, query1, query1text)\n",
    "print()\n",
    "\n",
    "check_relative_error(A, Ak_kai, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c58692b",
   "metadata": {},
   "source": [
    "### Entropy based selection\n",
    "\n",
    "The **entropy based selection** measures the degree to which the probability of the system is spread out over different possible microstates.\n",
    "\n",
    "Applying the entropy formula to the SVD matrices, we get that\n",
    "\n",
    "$$E(A)=-\\frac{1}{log(r)}\\sum_{j=1}^r f_j log(f_j)$$\n",
    "\n",
    "where \n",
    "\n",
    "$$f_j:=\\frac{\\sigma_j^2}{\\sum_{i=1}^r \\sigma_i^2}$$\n",
    "\n",
    "If $E(A)=1$, all the singular values give the same contribution, otherwise we can express all the important informations by the first eigenvectors.\n",
    "\n",
    "If we are interested in preserving the 90% by selecting the smallest $k$ such that $E_k(A) = 0.9E(A)$ we need to set $r=k$ in the first formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "333d6a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested k = 2\n",
      "\n",
      "Question: gradient\n",
      "\n",
      "Best answer\n",
      " The partial derivative is the ordinary derivative of a partial function\n",
      "\n",
      "Index of the document: 2\n",
      "\n",
      "Percentage relative error with k = 2 is 82.48566042563193 %\n"
     ]
    }
   ],
   "source": [
    "f = s**2 / np.sum(s**2)\n",
    "r = s.shape\n",
    "\n",
    "entropy = (-1/np.log(r))*np.sum(f*np.log(f))\n",
    "ks = int(r*entropy)\n",
    "\n",
    "perc = 1 - 0.90\n",
    "k = int(r*entropy*perc)\n",
    "\n",
    "print('Suggested k =', k)\n",
    "print()\n",
    "\n",
    "Ak_ent = low_rank_matrix(A, k)\n",
    "Ak_ent_cos_theta = compute_cosine_similarity(Ak_ent, query1, query1text)\n",
    "print()\n",
    "\n",
    "check_relative_error(A, Ak_ent, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b827d16b",
   "metadata": {},
   "source": [
    "### Variance based methods\n",
    "\n",
    "**Variance based methods** require to standardize data before using it. The comulative percentage of total variance allows to retain the first k singular values such that the cumulative percentage lies between 70%-90% such that\n",
    "\n",
    "$$\\frac{\\sum_{i=1}^k \\sigma_i^2}{\\sum_{i=1}^r \\sigma_i^2} > t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "266b18f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested k = 8\n",
      "\n",
      "Question: gradient\n",
      "\n",
      "Best answer\n",
      " A steepest descent method uses the opposite of the gradient direction\n",
      "\n",
      "Index of the document: 7\n",
      "\n",
      "Percentage relative error with k = 8 is 57.68982282414139 %\n"
     ]
    }
   ],
   "source": [
    "# compute the total variake of the singular values\n",
    "total_variance = np.sum(s**2)\n",
    "explained_variance = np.cumsum(s**2) / total_variance\n",
    "\n",
    "# choose to retain the 90% of total variance\n",
    "tolerance = 0.90\n",
    "k = np.sum(explained_variance > 0.90)\n",
    "\n",
    "print('Suggested k =', k)\n",
    "print()\n",
    "\n",
    "Ak_var = low_rank_matrix(A, k)\n",
    "Ak_var_cos_theta = compute_cosine_similarity(Ak_var, query1, query1text)\n",
    "print()\n",
    "\n",
    "check_relative_error(A, Ak_var, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a673d5b",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b5edad",
   "metadata": {},
   "source": [
    "All the results are reported in the following table:\n",
    "\n",
    "| Approach | Suggested $k$ | Relative error (%) | Index of retrieved document |\n",
    "|---|---|---|---|\n",
    "| Full matrix | 23 | 6e-13 | 7 |\n",
    "| Scree plot | 1 | 88.11 | 9 |\n",
    "| Kaiser rule | 9 | 53.61 | 7 |\n",
    "| Entropy | 2 | 82.48 | 2 |\n",
    "| Variance | 8 | 57.68 | 7 |\n",
    "\n",
    "Reading the result summarized in this table, we can appreciate how choosing $k = 8$ or $k = 9$ gives the same result for cosine similarity, with a relative error of $53-58\\%$ with respect to the original data of the SVD.\n",
    "\n",
    "On the other hand, choosing $k = 1$ or $k = 2$ will lead us to get an higher relative error of $82-88\\%$ with respect to the original data of the SVD.\n",
    "\n",
    "A further consideration has to be done when reading the plot made for the Scree Plot approach. By plotting the singular values $\\sigma_i$ obtained from the SVD we can notice how between the first singular value $\\sigma_0$ and the second one $\\sigma_1$ there is a quite big gap, suggesting that the majority of the information is held by its vectors.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c82ea0",
   "metadata": {},
   "source": [
    "To show the error with respect to the cosine similarity on the full matrix A, we apply again the cosine similarity between the vector with the $cos(\\theta)$ obtained from the full matrix $A$ and the $cos(\\theta_k)$ from the truncated SVD with $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e66e45f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_theta_norm = np.linalg.norm(A_full_cos_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ae82622c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scree plot error = [[0.13600049]]\n",
      "Kaiser error = [[0.73494415]]\n",
      "Entropy error = [[0.37689004]]\n",
      "Variance error = [[0.65642362]]\n"
     ]
    }
   ],
   "source": [
    "# Show the error with respect to the cosine similarity on full matrix A\n",
    "scree_plot_error = np.dot(A_full_cos_theta.T, Ak_sp_cos_theta) / (cos_theta_norm * np.linalg.norm(Ak_sp_cos_theta))\n",
    "kaiser_error = np.dot(A_full_cos_theta.T, Ak_kai_cos_theta) / (cos_theta_norm * np.linalg.norm(Ak_kai_cos_theta))\n",
    "entropy_error = np.dot(A_full_cos_theta.T, Ak_ent_cos_theta) / (cos_theta_norm * np.linalg.norm(Ak_ent_cos_theta))\n",
    "variance_error = np.dot(A_full_cos_theta.T, Ak_var_cos_theta) / (cos_theta_norm * np.linalg.norm(Ak_var_cos_theta))\n",
    "\n",
    "print('Scree plot error =', scree_plot_error)\n",
    "print('Kaiser error =', kaiser_error)\n",
    "print('Entropy error =', entropy_error)\n",
    "print('Variance error =', variance_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64392e4a",
   "metadata": {},
   "source": [
    "By reading these last results we can say that choosing $k = 9$ given by the **Kaiser rule approach**, will lead us to the best approximation for the truncated SVD.\n",
    "\n",
    "In this case, the Kaiser rule approach and the Variance approach have returned the same document related to our query, that is the same of the full matrix SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65d3b51",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "> Using the singular value decomposition, compute the closest point  to the query in the range of $A$ and in the range of $A_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adefe973",
   "metadata": {},
   "source": [
    "From [Task 3](#task-3), we decided to select $k=9$ has a good approximation of our low-rank matrix. In this task, we will use the low-rank matrix $A_k$ with $k=9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5d676e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.copy(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "804cfa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-rank matrix Ak shape (rows, cols) = (77, 23)\n"
     ]
    }
   ],
   "source": [
    "k = 9\n",
    "Ak = low_rank_matrix(A, k)\n",
    "\n",
    "print('Low-rank matrix Ak shape (rows, cols) =', Ak.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d86e8e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix rank = 23\n",
      "Low-rank matrix rank = 9\n"
     ]
    }
   ],
   "source": [
    "rank_a = np.linalg.matrix_rank(A)\n",
    "rank_ak = np.linalg.matrix_rank(Ak)\n",
    "\n",
    "print('Original matrix rank =', rank_a)\n",
    "print('Low-rank matrix rank =', rank_ak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "af14d00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R(A) shape (rows, cols) = (77, 23)\n",
      "R(Ak) shape (rows, cols) = (77, 9)\n"
     ]
    }
   ],
   "source": [
    "# Find the column space of A and the column space of Ak\n",
    "U, s, Vt = sp.linalg.svd(A, full_matrices=False)\n",
    "Uk, sk, Vtk = sp.linalg.svd(Ak, full_matrices=False)\n",
    "\n",
    "range_a = U[:, :rank_a]\n",
    "range_ak = Uk[:, :rank_ak]\n",
    "\n",
    "print('R(A) shape (rows, cols) =', range_a.shape)\n",
    "print('R(Ak) shape (rows, cols) =', range_ak.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27da2b9a",
   "metadata": {},
   "source": [
    "The **Closest Point Theorem** states that if we have a subspace $M$ of an inner-product space $V$ and a vector $b$ be a vector in $V$. The unique vector in $M$ that is closes to $b$ is given by\n",
    "\n",
    "$$p=P_Mb$$\n",
    "\n",
    "that is the orthogonal projection of $b$ onto $M$. In other words,\n",
    "\n",
    "$$\\min_{m \\in M}\\|b-m\\|_2=\\|b-P_Mb\\|_2 = dist(b, m)$$\n",
    "\n",
    "and this is called the orthogonal distance between $b$ and $M$.\n",
    "\n",
    "Since we have applied the SVD factorization on $A$ and $A_k$, and we have extracted $R(A)$ and $R(A_k)$ from $U$ and $U_k$, respectively the $U$ matrix of $A$ and $A_k$, we can now build the projector $P_{R(A)}$ and the projector $P_{R(A_k)}$ and check if our query vector $q$ satisfies:\n",
    "\n",
    "$$\\|q-P_{R(A)}q\\|_2 = dist(q, R(A))$$\n",
    "$$\\|q-P_{R(A_k)}q\\|_2 = dist(q, R(A_k))$$\n",
    "\n",
    "that is the orthogonal distance between $q$ and $P_{R(A)}$ and $q$ and $P_{R(A_k)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fdd5ea4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthogonal distance between q and R(A) = 8.66001657598723\n",
      "Orthogonal distance between q and R(Ak) = 8.681492277255412\n",
      "Absolute error = 0.02147570126818188\n"
     ]
    }
   ],
   "source": [
    "# Compute the projectors for R(A) and R(Ak)\n",
    "proj_range_a = np.dot(range_a, range_a.T)\n",
    "proj_range_ak = np.dot(range_ak, range_ak.T)\n",
    "\n",
    "# Compute closest point to the query in the range of A and Ak\n",
    "cpt_a = np.linalg.norm((query1 - proj_range_a*query1), 2)\n",
    "cpt_ak = np.linalg.norm((query1 - proj_range_ak*query1), 2)\n",
    "\n",
    "print('Orthogonal distance between q and R(A) =', cpt_a)\n",
    "print('Orthogonal distance between q and R(Ak) =', cpt_ak)\n",
    "print('Absolute error =', np.abs(cpt_a - cpt_ak))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0630501a",
   "metadata": {},
   "source": [
    "Choosing $A_k$ with $k = 9$ we are actually using a good approximation of the full matrix $A$, as we can see from the absolute error between the orthogonal distances that we have computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f77b77b",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "\n",
    "> Choose another query vector ``query2`` at your discretion and repeat the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d5dd5b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.copy(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "71b6b2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-norm of query2 = 1.0\n"
     ]
    }
   ],
   "source": [
    "# query vector\n",
    "query2text = ['spectrum']\n",
    "query_stem = query2text[0]\n",
    "query2 = []\n",
    "query2.append(query_stem)\n",
    "query2 = vectorizer.transform(query2).toarray()\n",
    "\n",
    "# normalize query vector\n",
    "q = np.linalg.norm((query2), 2)\n",
    "print('2-norm of query2 =', q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c204acd",
   "metadata": {},
   "source": [
    "For this task, we will compute the following:\n",
    "\n",
    "1. Classic cosine similarity\n",
    "2. Cosine similarity with QR decomposition\n",
    "3. Cosine similarity with SVD\n",
    "4. Cosine similarity with truncated SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3256e3b",
   "metadata": {},
   "source": [
    "### Classic cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7ef1124e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: spectrum\n",
      "\n",
      "Best answer\n",
      " Similar matrices have the same spectrum.\n",
      "\n",
      "Index of the document: 20\n",
      "\n",
      "Cosine of the angle between the query and the documents\n",
      " [[0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.40824829]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.57735027]\n",
      " [0.        ]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# we do the transpose of the matrix and the vector so they are conformable for matrix-vector product\n",
    "cos_theta = compute_cosine_similarity(A, query2, query2text)\n",
    "\n",
    "print('\\nCosine of the angle between the query and the documents\\n', cos_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1304a364",
   "metadata": {},
   "source": [
    "### Cosine similarity with QR decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6e0a15b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: spectrum\n",
      "\n",
      "Best answer\n",
      " The eigenvalues of a matrix are the roots of the characteristic polynomial.\n",
      "\n",
      "Index of the document: 3\n",
      "\n",
      "Cosine of the angle between the query and the documents\n",
      " [[ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.57735027]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# we do the transpose of the matrix and the vector so they are conformable for matrix-vector product\n",
    "cos_theta_qr = compute_cosine_similarity(orth_basis, query2, query2text)\n",
    "\n",
    "print('\\nCosine of the angle between the query and the documents\\n', cos_theta_qr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98876402",
   "metadata": {},
   "source": [
    "### Cosine similarity with SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "08493a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: spectrum\n",
      "\n",
      "Best answer\n",
      " Similar matrices have the same spectrum.\n",
      "\n",
      "Index of the document: 20\n",
      "\n",
      "Cosine of the angle between the query and the documents\n",
      " [[ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.        ]\n",
      " [ 0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.40824829]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.57735027]\n",
      " [-0.        ]\n",
      " [-0.        ]]\n"
     ]
    }
   ],
   "source": [
    "k = np.linalg.matrix_rank(A)\n",
    "\n",
    "Ak_full = low_rank_matrix(A, k)\n",
    "Ak_full_cos_theta = compute_cosine_similarity(Ak_full, query2, query2text)\n",
    "print('\\nCosine of the angle between the query and the documents\\n', Ak_full_cos_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53b10bd",
   "metadata": {},
   "source": [
    "### Cosine similarity with truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "33b109a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: spectrum\n",
      "\n",
      "Best answer\n",
      " Similar matrices have the same spectrum.\n",
      "\n",
      "Index of the document: 20\n",
      "\n",
      "Cosine of the angle between the query and the documents\n",
      " [[-0.00743552]\n",
      " [ 0.01713358]\n",
      " [ 0.00229594]\n",
      " [-0.0275338 ]\n",
      " [ 0.0072565 ]\n",
      " [ 0.02789349]\n",
      " [ 0.03157814]\n",
      " [ 0.00129189]\n",
      " [ 0.01372687]\n",
      " [-0.01630155]\n",
      " [ 0.00159507]\n",
      " [-0.03072833]\n",
      " [-0.01106117]\n",
      " [ 0.05567374]\n",
      " [ 0.01810512]\n",
      " [-0.00743552]\n",
      " [ 0.10634069]\n",
      " [ 0.1728674 ]\n",
      " [ 0.05499462]\n",
      " [-0.04786427]\n",
      " [ 0.40947313]\n",
      " [-0.00166075]\n",
      " [ 0.29130164]]\n"
     ]
    }
   ],
   "source": [
    "k = 9\n",
    "\n",
    "Ak = low_rank_matrix(A, k)\n",
    "Ak_cos_theta = compute_cosine_similarity(Ak, query2, query2text)\n",
    "print('\\nCosine of the angle between the query and the documents\\n', Ak_cos_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed1471f",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The final results for this task are reported in the following table\n",
    "\n",
    "| | Technique | Index of retrieved document |\n",
    "|---|---|---|\n",
    "| 1 | Classic cosine similarity | 20 |\n",
    "| 2 | Cosine similarity with QR decomposition | 3 |\n",
    "| 3 | Cosine similarity with SVD | 20 |\n",
    "| 4 | Cosine similarity with truncated SVD | 20 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efaa772",
   "metadata": {},
   "source": [
    "## Task 6\n",
    "\n",
    "> Construct the column-covariance matrix, numerically determine if it is positive definite. Do the eigenvalues of such a matrix satisfy the theoretical relation with the singular value of the matrix $A$? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bbc6d2",
   "metadata": {},
   "source": [
    "To construct the column-covariance matrix, we first need to center our data such that the mean of the columns of our matrix $A$ is equal to $0$.\n",
    "\n",
    "$$X = A - mean(A)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5b4acfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.copy(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e25345ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# center original data from A\n",
    "X = A - np.mean(A, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb321332",
   "metadata": {},
   "source": [
    "Now that our data is centred, we can compute our covariance matrix using matrices obtained with the **Principal Component Analysis (PCA)**, that is a SVD but with a centred matrix $X$ instead of the classic $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "74e7d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, Vt = sp.linalg.svd(X, full_matrices=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee480c",
   "metadata": {},
   "source": [
    "By definition\n",
    "\n",
    "$$Cov(A) = \\frac{X^T X}{m}$$\n",
    "\n",
    "where $X$ is our centred matrix and $m$ is the number of rows of the matrix. For the unbiased version of the formula, we change the denominator such that\n",
    "\n",
    "$$Cov(A) = \\frac{X^T X}{m - 1}$$\n",
    "\n",
    "From the SVD decomposition, we can rewrite $Cov(A)$ as it follows\n",
    "\n",
    "$$Cov(A) = \\frac{X^T X}{m} = \\frac{V \\Sigma^T U^T U \\Sigma V^T}{m} = \\frac{V \\Sigma^2 V^T}{m}$$\n",
    "\n",
    "so we need $V^T$ from the SVD on the centred matrix X to get the covariance matrix.\n",
    "\n",
    "For this task we will compute the column-covariance matrix with both methods, to check also the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "06aa0492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first method\n",
    "m = X.shape[0]\n",
    "cov_1 = np.dot(X.T, X) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6dac431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second method\n",
    "cov_2 = np.dot(np.dot(Vt.T, np.diag(s**2)), Vt) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0a848665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both methods for computing the covariance matrix return the same matrix\n",
      " True\n"
     ]
    }
   ],
   "source": [
    "check_cov = np.all(np.isclose(cov_1, cov_2))\n",
    "print('Both methods for computing the covariance matrix return the same matrix\\n', check_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6b06f3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix\n",
      " [[ 0.01163771  0.00047331  0.00037757  0.0025805   0.00027408  0.00070599\n",
      "  -0.00111312 -0.00126215  0.00009939  0.00314913  0.00304014  0.00098671\n",
      "   0.00347849  0.00070599  0.0025805   0.01001434  0.00223478  0.0025805\n",
      "   0.00194795 -0.00100685 -0.00082627  0.00189745  0.00070599]\n",
      " [ 0.00047331  0.01180638  0.00201819  0.00091088  0.00220877  0.00091088\n",
      "   0.00059499 -0.00118064  0.00029749  0.00042334  0.00119739 -0.00099782\n",
      "   0.00059499  0.00291482  0.00291482  0.00047331 -0.00094662  0.00091088\n",
      "   0.00059499  0.00201819 -0.00077291  0.00220877 -0.00109306]\n",
      " [ 0.00037757  0.00201819  0.0122357   0.00072663  0.00729967  0.007121\n",
      "   0.00047463 -0.00094182  0.00023732  0.00529076  0.00095518 -0.00079599\n",
      "   0.00047463  0.00072663  0.00072663  0.00037757 -0.00075514  0.00072663\n",
      "   0.00047463 -0.00075131 -0.00061657  0.00176199 -0.00087196]\n",
      " [ 0.0025805   0.00091088  0.00072663  0.01197504  0.00070599  0.00115253\n",
      "  -0.00096399 -0.00109306  0.0005279   0.00398469  0.00381838  0.00144729\n",
      "   0.00433794  0.00115253  0.00331703  0.0025805  -0.0008764   0.00331703\n",
      "   0.00257063 -0.00087196 -0.00071557  0.0025805   0.00115253]\n",
      " [ 0.00027408  0.00220877  0.00729967  0.00070599  0.01163771  0.00445501\n",
      "   0.00041742  0.00047331  0.00009939  0.00314913  0.00098671 -0.00106672\n",
      "   0.00041742  0.00070599  0.00070599  0.00027408 -0.00101198  0.00070599\n",
      "   0.00041742 -0.00100685 -0.00082627  0.00189745 -0.00116853]\n",
      " [ 0.00070599  0.00091088  0.007121    0.00115253  0.00445501  0.01197504\n",
      "  -0.00096399 -0.00109306  0.0005279   0.00398469  0.00144729 -0.0009238\n",
      "   0.00080332  0.00548153  0.00115253  0.00070599 -0.0008764   0.00115253\n",
      "   0.00080332 -0.00087196 -0.00071557  0.00070599 -0.00101198]\n",
      " [-0.00111312  0.00059499  0.00047463 -0.00096399  0.00041742 -0.00096399\n",
      "   0.01206874 -0.00104122  0.00026236 -0.0009956  -0.00088     0.00105599\n",
      "  -0.00091827 -0.00096399 -0.00096399 -0.00111312 -0.00083484 -0.00096399\n",
      "  -0.00091827 -0.00083061 -0.00068164  0.00194795 -0.00096399]\n",
      " [-0.00126215 -0.00118064 -0.00094182 -0.00109306  0.00047331 -0.00109306\n",
      "  -0.00104122  0.01180638  0.00356991 -0.00112891 -0.00099782 -0.00099782\n",
      "  -0.00104122 -0.00109306 -0.00109306 -0.00126215 -0.00094662 -0.00109306\n",
      "  -0.00104122 -0.00094182 -0.00077291 -0.00126215 -0.00109306]\n",
      " [ 0.00009939  0.00029749  0.00023732  0.0005279   0.00009939  0.0005279\n",
      "   0.00026236  0.00356991  0.01146905  0.00008889  0.00080457 -0.00113142\n",
      "   0.00026236  0.0005279   0.0005279   0.00009939 -0.00107336  0.0005279\n",
      "   0.00026236 -0.00106793 -0.0008764   0.00009939 -0.00123941]\n",
      " [ 0.00314913  0.00042334  0.00529076  0.00398469  0.00314913  0.00398469\n",
      "  -0.0009956  -0.00112891  0.00008889  0.01190757  0.00455582  0.00271918\n",
      "   0.00584916  0.00063145  0.00398469  0.00314913 -0.00090514  0.00398469\n",
      "   0.00311125 -0.00090055 -0.00073904  0.00460112  0.00230807]\n",
      " [ 0.00304014  0.00119739  0.00095518  0.00381838  0.00098671  0.00144729\n",
      "  -0.00088    -0.00099782  0.00080457  0.00455582  0.0121437   0.00175409\n",
      "   0.00686396  0.00144729  0.00381838  0.00304014  0.00125339  0.00381838\n",
      "   0.00686396 -0.00079599 -0.00065323  0.00304014  0.00144729]\n",
      " [ 0.00098671 -0.00099782 -0.00079599  0.00144729 -0.00106672 -0.0009238\n",
      "   0.00105599 -0.00099782 -0.00113142  0.00271918  0.00175409  0.0121437\n",
      "   0.00299198 -0.0009238   0.00144729  0.00098671 -0.00080004  0.00144729\n",
      "   0.00105599 -0.00079599 -0.00065323  0.00098671  0.00144729]\n",
      " [ 0.00347849  0.00059499  0.00047463  0.00433794  0.00041742  0.00080332\n",
      "  -0.00091827 -0.00104122  0.00026236  0.00584916  0.00686396  0.00299198\n",
      "   0.01206874  0.00080332  0.00433794  0.00347849  0.0006957   0.00433794\n",
      "   0.00629673  0.00047463 -0.00068164  0.00347849  0.00257063]\n",
      " [ 0.00070599  0.00291482  0.00072663  0.00115253  0.00070599  0.00548153\n",
      "  -0.00096399 -0.00109306  0.0005279   0.00063145  0.00144729 -0.0009238\n",
      "   0.00080332  0.01197504  0.00115253  0.00070599 -0.0008764   0.00115253\n",
      "   0.00080332 -0.00087196 -0.00071557  0.00070599 -0.00101198]\n",
      " [ 0.0025805   0.00291482  0.00072663  0.00331703  0.00070599  0.00115253\n",
      "  -0.00096399 -0.00109306  0.0005279   0.00398469  0.00381838  0.00144729\n",
      "   0.00433794  0.00115253  0.01197504  0.0025805  -0.0008764   0.00331703\n",
      "   0.00433794  0.00232522 -0.00071557  0.0025805   0.00331703]\n",
      " [ 0.01001434  0.00047331  0.00037757  0.0025805   0.00027408  0.00070599\n",
      "  -0.00111312 -0.00126215  0.00009939  0.00314913  0.00304014  0.00098671\n",
      "   0.00347849  0.00070599  0.0025805   0.01163771  0.00223478  0.0025805\n",
      "   0.00194795 -0.00100685 -0.00082627  0.00189745  0.00070599]\n",
      " [ 0.00223478 -0.00094662 -0.00075514 -0.0008764  -0.00101198 -0.0008764\n",
      "  -0.00083484 -0.00094662 -0.00107336 -0.00090514  0.00125339 -0.00080004\n",
      "   0.0006957  -0.0008764  -0.0008764   0.00223478  0.01222803  0.00287263\n",
      "   0.0052873  -0.00075514 -0.00061971 -0.00101198 -0.0008764 ]\n",
      " [ 0.0025805   0.00091088  0.00072663  0.00331703  0.00070599  0.00115253\n",
      "  -0.00096399 -0.00109306  0.0005279   0.00398469  0.00381838  0.00144729\n",
      "   0.00433794  0.00115253  0.00331703  0.0025805   0.00287263  0.01197504\n",
      "   0.00433794 -0.00087196  0.00234549  0.0025805   0.00115253]\n",
      " [ 0.00194795  0.00059499  0.00047463  0.00257063  0.00041742  0.00080332\n",
      "  -0.00091827 -0.00104122  0.00026236  0.00311125  0.00686396  0.00105599\n",
      "   0.00629673  0.00080332  0.00433794  0.00194795  0.0052873   0.00433794\n",
      "   0.01206874 -0.00083061 -0.00068164  0.00194795  0.00080332]\n",
      " [-0.00100685  0.00201819 -0.00075131 -0.00087196 -0.00100685 -0.00087196\n",
      "  -0.00083061 -0.00094182 -0.00106793 -0.00090055 -0.00079599 -0.00079599\n",
      "   0.00047463 -0.00087196  0.00232522 -0.00100685 -0.00075514 -0.00087196\n",
      "  -0.00083061  0.0122357  -0.00061657 -0.00100685 -0.00087196]\n",
      " [-0.00082627 -0.00077291 -0.00061657 -0.00071557 -0.00082627 -0.00071557\n",
      "  -0.00068164 -0.00077291 -0.0008764  -0.00073904 -0.00065323 -0.00065323\n",
      "  -0.00068164 -0.00071557 -0.00071557 -0.00082627 -0.00061971  0.00234549\n",
      "  -0.00068164 -0.00061657  0.01248103 -0.00082627  0.00540656]\n",
      " [ 0.00189745  0.00220877  0.00176199  0.0025805   0.00189745  0.00070599\n",
      "   0.00194795 -0.00126215  0.00009939  0.00460112  0.00304014  0.00098671\n",
      "   0.00347849  0.00070599  0.0025805   0.00189745 -0.00101198  0.0025805\n",
      "   0.00194795 -0.00100685 -0.00082627  0.01163771  0.00070599]\n",
      " [ 0.00070599 -0.00109306 -0.00087196  0.00115253 -0.00116853 -0.00101198\n",
      "  -0.00096399 -0.00109306 -0.00123941  0.00230807  0.00144729  0.00144729\n",
      "   0.00257063 -0.00101198  0.00331703  0.00070599 -0.0008764   0.00115253\n",
      "   0.00080332 -0.00087196  0.00540656  0.00070599  0.01197504]]\n"
     ]
    }
   ],
   "source": [
    "print('Covariance matrix\\n', cov_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "812db193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The covariance matrix is positive definite\n",
      " True\n"
     ]
    }
   ],
   "source": [
    "# Determine if it is positive definite\n",
    "Ucov, scov, Vtcov = sp.linalg.svd(cov_1, full_matrices=False)\n",
    "\n",
    "is_positive_def_2 = np.all(scov > 0)\n",
    "print('The covariance matrix is positive definite\\n', is_positive_def_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd55375",
   "metadata": {},
   "source": [
    "There exists a theoretical relation with the **eigenvalues of the covariance matrix** $Cov(A)$ and the **singular values of the centred matrix** $X$.\n",
    "\n",
    "We can compute the eigenvalues $\\lambda_i$ of the covariance matrix $Cov(A)$ by doing an SVD decomposition, because it is known that\n",
    "\n",
    "$$\\sigma_i = \\sqrt{\\lambda_i}$$\n",
    "\n",
    "$$\\sigma_i^2 = \\lambda_i$$\n",
    "\n",
    "By definition, when applying an SVD decomposition, we get the matrix $\\Sigma$ that contains all the $\\sigma_i$ that we need to compute the eigenvalues of the decomposed matrix as the following\n",
    "\n",
    "$$\\Sigma^2 = \\Lambda$$\n",
    "\n",
    "In this case, we do not need to compute the $\\Sigma^2$ because the covariance matrix is a **symmetric matix** that satisfies this property\n",
    "\n",
    "$$A = A^T$$\n",
    "$$A^T A = AA^T$$\n",
    "$$A^T A = V^T \\Sigma^2 V = U \\Sigma^2 U^T = A A^T$$\n",
    "\n",
    "If a matrix is symmetric, then $U = V$, by the spectral theorem, we can rewrite $Cov(A)$ as \n",
    "\n",
    "$$Cov(A) = \\frac{X^T X}{m} = \\frac{X^2}{m} = \\frac{V \\Lambda V^T}{m} = \\frac{U \\Lambda U^T}{m}$$\n",
    "\n",
    "So we are sure that the singular values (that are also its eigenvalues) of the covariance matrix $Cov(A)$ are equal to its $\\frac{\\Lambda}{m}$, without performing the 2-power of the values inside it and they are the same as the $\\frac{\\Sigma^2}{m}$ from the SVD of $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6efe852c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular values of the covariance matrix\n",
      " [0.0500145  0.02827393 0.01996833 0.01772475 0.01688504 0.01672103\n",
      " 0.0158126  0.01356396 0.01314255 0.00979707 0.0095335  0.008861\n",
      " 0.00836153 0.00782761 0.00707323 0.0059435  0.00530644 0.00504909\n",
      " 0.00441304 0.00367575 0.00280995 0.00268276 0.00162338]\n"
     ]
    }
   ],
   "source": [
    "cov_sv = scov\n",
    "print('Singular values of the covariance matrix\\n', cov_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "30713c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues of the centred matrix\n",
      " [0.0500145  0.02827393 0.01996833 0.01772475 0.01688504 0.01672103\n",
      " 0.0158126  0.01356396 0.01314255 0.00979707 0.0095335  0.008861\n",
      " 0.00836153 0.00782761 0.00707323 0.0059435  0.00530644 0.00504909\n",
      " 0.00441304 0.00367575 0.00280995 0.00268276 0.00162338]\n"
     ]
    }
   ],
   "source": [
    "X_eig_val = (s**2) / X.shape[0]\n",
    "print('Eigenvalues of the centred matrix\\n', X_eig_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fa40ad56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The eigenvalues of the covariance matrix have a relation with the singular values of X\n",
      " True\n"
     ]
    }
   ],
   "source": [
    "relation_svd_eig = np.all(np.isclose(cov_sv, X_eig_val))\n",
    "print('The eigenvalues of the covariance matrix have a relation with the singular values of X\\n', relation_svd_eig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
